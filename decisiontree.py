#
#   Decision tree algorithm using pandas and sklearn (scikit-learn)
#  
#   Creates a Decision Tree based on the contents of "data.csv" (or otherwise specified), 
#   and saves a visualization of the tree in "tree.png" (or otherwise specified).
#   If specified, runs tests and save results to "results.csv" (or otherwise specified).
#   Also saves an image of the Macro ROC Curve generated by the tree in "macro_roc.png".
#
# TODO: maybe rename the command line arguments?

import os
import time
import argparse
import pandas
import numpy as np
from sklearn import tree, metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt
import graphviz


DEPTH_ABSOLUTE_MAXIMUM = 100    # Change this for your limit when running bestDepth() or minDepth()

# Additional functions for choosing tree depth
# Tries every depth from i to DEPTH_ABSOLUTE_MAXIMUM and returns the tree with best accuracy
def bestDepth(X_train, X_test, y_train, y_test):
    best_acc = 0.0
    for i in range(1, DEPTH_ABSOLUTE_MAXIMUM):
        dtree = DecisionTreeClassifier(max_depth=i)
        dtree = dtree.fit(X_train, y_train)
        y_pred = dtree.predict(X_test)
        acc = metrics.accuracy_score(y_test, y_pred)
        # print(f"[{i}]: {acc}")

        if (acc > best_acc):
            best_acc = acc
            best_depth = i
            best_tree = dtree

    print(f"Best depth: {best_depth}, with Accuracy of: {best_acc:.4f}.")
    return best_tree

# Tries every depth from DEPTH_ABSOLUTE_MAXIMUM to i and returns the last tree with > 0.85 (defined in argparse in main())
def minDepth(X_train, X_test, y_train, y_test, args):
    print(f"Using minimum accuracy of: {args.accuracy}")
    best_tree = None
    best_depth = 0
    best_acc = 0.0
    
    for current_depth in range(1, DEPTH_ABSOLUTE_MAXIMUM):
        dtree = DecisionTreeClassifier(max_depth=current_depth)
        dtree = dtree.fit(X_train, y_train)
        y_pred = dtree.predict(X_test)
        acc = metrics.accuracy_score(y_test, y_pred)
        # print(f"[{i}]: {acc}")

        # If got to the target accuracy, saves and ends
        if acc >= args.accuracy:
            best_tree = dtree
            best_depth = current_depth
            best_acc = acc
            break
        # Else, keep running 
        elif acc > best_acc or best_tree is None:
            best_tree = dtree
            best_depth = current_depth
            best_acc = acc
    
    # After checking all depths or finding the first suitable one
    if best_acc >= args.accuracy:
        print(f"Minimum depth: {best_depth}, with Accuracy of : {best_acc:.4f};")
    else:
        print(f"No tree met accuracy requirement. Returning best found. Depth: {best_depth}, with Accuracy of {best_acc:.4f}.")
    
    return best_tree

def drawGraphGraphviz(dtree, attributes, args):
    dot_data = tree.export_graphviz(
                                dtree, out_file=None, feature_names=attributes,
                                filled=True
                                )

    # Change this to be the one specified by the user:
    filename, extension = os.path.splitext(args.output)
    extension = extension[1:]
    # Render
    graph = graphviz.Source(dot_data, format=extension)
    graph.render(filename)
    os.remove(filename)

def drawGraphMatplot(dtree, attributes, args):
    tree.plot_tree(
                dtree, feature_names=attributes, 
                filled=True
                )
    # Render
    plt.savefig(args.output, dpi=700)
    plt.close()

def saveRocCurve(y_onehot_test, y_score, macro_roc_auc_ovr):
    RocCurveDisplay.from_predictions(
        y_onehot_test.ravel(),
        y_score.ravel(),
        name=f"Macro-average ROC",
        color= "darkviolet",
        plot_chance_level=True,
    )
    plt.savefig("macro_roc.png")
    plt.close()

def saveResults(results, args):
    # Calculate averages
    averages = results.mean(numeric_only=True).to_frame().T 
    averages['Run'] = 'Average'

    # Add to the final file, save as .csv
    final_results = pandas.concat([averages, results], ignore_index=True)
    final_results.to_csv(args.metric, index=False)
    
    # Summary just because
    print("\nBenchmark Summary:")
    print(f"Average training time: {averages['training_time'].values[0]:.4f} seconds")
    print(f"Average testing time: {averages['testing_time'].values[0]:.4f} seconds")
    print(f"Average total time: {averages['total_time'].values[0]:.4f} seconds")
    print(f"Average accuracy: {averages['accuracy'].values[0]:.4f}")
    print(f"Average F-measure: {averages['f-measure'].values[0]:.4f}")


def runBenchmark(X, y, args):
    # Our ""defines""
    MAX_RUN = args.n__number_runs          # How many runs to do
    MAGIC_NUMBER = 39       # Doesn't really matter. More for flare, to be honest
    R_FOLDS = 10

    results = []  
    # Saves ROC values for each run (average found within folds)
    roc_values = {
        'run': [],
        'roc_auc_ovr': [],
        'y_score': [],
        'y_onehot': [],
    }
    
    # Split into 70/30 (or whichever desired) before proceeding
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test, random_state=39)

    # Do runs
    print(f"Executing {MAX_RUN} runs:")
    for i in range(MAX_RUN):
        # Store the fold metrics for each iteration
        fold_metrics = {
            'training_time': [],
            'testing_time': [],
            'accuracy': [],
            'f_measure': [],
            'roc_auc_ovr': [],
            'y_score': [],
            'y_onehot': [],
        }
        
        # print(f"Creating {R_FOLDS}-fold cross-validation.")
        skf = StratifiedKFold(n_splits=R_FOLDS)
        # print(f"Starting run {i+1}:")
        for train_index, test_index in skf.split(X_train, y_train): 
            fold_X_train, fold_X_test = X_train[train_index], X_train[test_index]
            fold_y_train, fold_y_test = y_train[train_index], y_train[test_index]
            
            dtree = DecisionTreeClassifier(random_state=MAGIC_NUMBER+i)

            # Benchmark training
            start_training = time.time()
            dtree.fit(fold_X_train, fold_y_train)
            training_time = time.time() - start_training

            # Benchmark testing
            start_testing = time.time()
            y_pred = dtree.predict(fold_X_test)
            testing_time = time.time() - start_testing

            # Metrics
            acc = metrics.accuracy_score(fold_y_test, y_pred)
            f_measure = metrics.f1_score(fold_y_test, y_pred, average='weighted')

            # ROC Curve
                # Initialize
            y_score = dtree.predict_proba(fold_X_test)
            label_binarizer = LabelBinarizer().fit(fold_y_train)
            y_onehot_test = label_binarizer.transform(fold_y_test)
                # Using OvR macro-average
            macro_roc_auc_ovr = roc_auc_score(
                fold_y_test,
                y_score,
                multi_class="ovr",
                average="macro",
            )
            # These fold metrics are for deciding the average ROC curve from the folds.
            # print(f"\t\tRan a fold: {train_index+1}")
            fold_metrics['roc_auc_ovr'].append(macro_roc_auc_ovr)
            fold_metrics['y_score'].append(y_score)
            fold_metrics['y_onehot'].append(y_onehot_test)


            # Store the fold metrics into the array
            fold_metrics['training_time'].append(training_time)
            fold_metrics['testing_time'].append(testing_time)
            fold_metrics['accuracy'].append(acc)
            fold_metrics['f_measure'].append(f_measure)

        # Select the average ROC curve for the fold based on the fold's accuracy
        mean_accuracy = np.mean(fold_metrics['accuracy'])
        # print(f"\tIteration {i+1}'s avg accuracy was: {mean_accuracy}")
            # list all accuracies with acc <= mean_acc
        lower_accuracies = [
            acc for acc in fold_metrics['accuracy'] if acc <= mean_accuracy
        ]
        closest_lower = max(lower_accuracies)
            # for entries with same acc (possible)
        closest_lower_items = [
            i for i, acc in enumerate(fold_metrics['accuracy']) if acc == closest_lower
        ]
        closest_lower_index = closest_lower_items[0]
        # print(f"\t\tThis iteration's closest acc was: {fold_metrics['accuracy'][closest_lower_index]}")
            # Save to this iteration's avg ROC
        roc_values['run'].append(i+1)
        roc_values['roc_auc_ovr'].append(fold_metrics['roc_auc_ovr'][closest_lower_index])
        roc_values['y_score'].append(fold_metrics['y_score'][closest_lower_index])
        roc_values['y_onehot'].append(fold_metrics['y_onehot'][closest_lower_index])

        # Save results
        results.append({
            'run': i+1,
            'training_time': np.mean(fold_metrics['training_time']),
            'testing_time': np.mean(fold_metrics['testing_time']),
            'total_time': np.mean(fold_metrics['training_time']) + np.mean(fold_metrics['testing_time']),
            'accuracy': mean_accuracy,
            'f-measure': np.mean(fold_metrics['f_measure']),
        })

    # Generate our average ROC
    # Select the average ROC curve for the iteration based on their roc_auc_ovr score
    mean_accuracy = np.mean(roc_values['roc_auc_ovr'])
        # list all accuracies with acc <= mean_acc
    lower_accuracies = [
        acc for acc in roc_values['roc_auc_ovr'] if acc <= mean_accuracy
    ]
    closest_lower = max(lower_accuracies)
        # for entries with same acc (possible)
    closest_lower_items = [
        i for i, acc in enumerate(roc_values['roc_auc_ovr']) if acc == closest_lower
    ]
    closest_lower_index = closest_lower_items[0]
    # Create the roc image
    print(f"Creating ROC curve with the values from the Average Iteration's Average Fold results, from run {roc_values['run'][closest_lower_index]}")
    saveRocCurve(roc_values['y_onehot'][closest_lower_index], roc_values['y_score'][closest_lower_index], roc_values['roc_auc_ovr'][closest_lower_index])


    # Send to final processing
    saveResults(pandas.DataFrame(results), args)






def main():
    parser = argparse.ArgumentParser(
        description="Generates a Decision Tree for a given dataset. Saves metrics and a visualization of the working tree.",
        epilog='Example: python decisiontree.py --depth best --test 0.4 --file "data processed.csv" --output output.pdf --benchmark --metric results.csv'
        )

    parser.add_argument('-d', '--depth',
                        choices=['best', 'minimum', 'limited'],
                        help='Depth selection: "best" for the best accuracy found, "minimum" for the lowest with accuracy still greater than 0.85 or that set by --accuracy, "limited" for a limited depth level specified by --limit. Default renders to the maximum depth.'
                        )
    parser.add_argument('-a', '--accuracy',
                        type=float,
                        help='Minimum accuracy desired when running "--depth minimum". Default is 0.85',
                        default=0.85
                        )
    parser.add_argument('-l', '--limit',
                        type=int,
                        help='Maximum depth level when running "--depth limited". Default is 15',
                        default=15
                        )
    parser.add_argument('-t', '--test',
                        type=float,
                        help='Test subset ratio size. Default is 0.3 for 30%% test, 70%% train.',
                        default=0.3
                        )
    parser.add_argument('-r', '--render',
                        choices=['viz', 'plt'],
                        help='Which tool to use to render the tree image. "viz" for Graphviz (default), "plt" for Matplotlib.',
                        default='viz'
                        )
    parser.add_argument('-f', '--file',
                        help='Path to dataset file. Must be .csv',
                        default='data.csv'
                        )
    parser.add_argument('-o', '--output',
                        help='Path to the output image file. Default is \'tree.png\'. Can also be other extensions, such as png and jpeg.',
                        default='tree.png'
                        )
    parser.add_argument('-b', '--benchmark',
                        action='store_true',
                        help='Ignores depth specifications and runs code multiple times to find metrics. Save them to the file specified in -m.'
                        )
    parser.add_argument('-n' '--number-runs',
                        type=int,
                        help='How many runs to do when executing "--benchmark". Default is 250.',
                        default=250
                        )
    parser.add_argument('-m', '--metric',
                        help='Path to the metrics output file. Default is \'metrics.csv\'.',
                        default='metrics.csv'
                        )

    args = parser.parse_args()


    # Read csv
    if not os.path.exists(args.file):
        print(f"Error: File '{args.file}' does not exist")
        exit(1)
    df = pandas.read_csv(args.file)

    # Change from string to numerical
    d = {'Hazardous': 0, 'Poor': 1, 'Moderate': 2, 'Good': 3}
    df['Air Quality'] = df['Air Quality'].map(d)
    attributes =['Temperature', 'Humidity', 'PM2.5', 'PM10', 'NO2', 'SO2', 'CO', 'Proximity_to_Industrial_Areas', 'Population_Density']

    # X are the attributes we use to predict Y
    X = df[attributes].values
    y = df['Air Quality'].values

    if (args.benchmark):
        runBenchmark(X, y, args)
        exit(0)

    # Divide into train and test subsets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test, random_state=39)


    # Create tree image and save it
    match args.depth:
        case "best":
            print("Mode: Best")
            dtree = bestDepth(X_train, X_test, y_train, y_test)
        case "minimum":
            print("Mode: Minimum")
            dtree = minDepth(X_train, X_test, y_train, y_test, args)
        case "limited":
            print("Mode: Limited")
            print(f"Using maximum depth of: {args.limit}")
            dtree = DecisionTreeClassifier(max_depth=args.limit)
            dtree = dtree.fit(X_train, y_train)
            y_pred = dtree.predict(X_test)
            print(f"Depth: {args.limit}, with Accuracy of: {metrics.accuracy_score(y_test, y_pred):.4f}.")
        case _:
            print("Mode: Default")
            dtree = DecisionTreeClassifier()
            dtree = dtree.fit(X_train, y_train)
            y_pred = dtree.predict(X_test)
            print(f"Accuracy: {metrics.accuracy_score(y_test, y_pred):.4f}")
            

    # ROC Curve
        # Initialize
    y_score = dtree.predict_proba(X_test)
    label_binarizer = LabelBinarizer().fit(y_train)
    y_onehot_test = label_binarizer.transform(y_test)
        # Using OvR macro-average
    macro_roc_auc_ovr = roc_auc_score(
        y_test,
        y_score,
        multi_class="ovr",
        average="macro",
    )
    print(f"Macro-averaged One-vs-Rest ROC AUC score:\n{macro_roc_auc_ovr:.2f}")
    saveRocCurve(y_onehot_test, y_score, macro_roc_auc_ovr)

    # saves image to the output file
    match args.render:
        case "viz":
            print("Render: Graphviz")
            drawGraphGraphviz(dtree, attributes, args)
        case "plt":
            print("Render: Matplotlib")
            drawGraphMatplot(dtree, attributes, args)
        case _:
            print(f"Unexpected render value: {args.render}. Rendering with Graphviz.")
            drawGraphGraphviz(dtree, attributes, args)


            
    

if __name__ == '__main__':
    main()